\section{Xây dựng và quản lý cơ sở dữ liệu vector}

\subsection{Tiền xử lý và phân đoạn văn bản}

Sau khi trích xuất được nội dung thô từ các nguồn tài liệu khác nhau, bước tiếp theo trong quy trình xây dựng hệ thống RAG là chuyển đổi văn bản dài thành các đoạn nhỏ phù hợp để phục vụ lưu trữ và tìm kiếm. Quá trình phân đoạn văn bản \emph{(text chunking)} đòi hỏi sự cân bằng: nếu đoạn quá ngắn, ngữ cảnh có thể bị rời rạc và khó hiểu đầy đủ ý nghĩa; ngược lại, nếu đoạn quá dài, mô hình embedding sẽ khó nắm bắt toàn bộ thông tin, từ đó làm giảm độ chính xác khi truy vấn.

Hệ thống sử dụng chiến lược phân đoạn dựa trên số lượng ký tự cố định với các tham số đã được tinh chỉnh: mỗi đoạn có kích thước tối đa (\texttt{chunk\_size}) \(8\,000\) ký tự và độ chồng lấn (\texttt{chunk\_overlap}) 800 ký tự giữa các đoạn liền kề. Ngưỡng \(8\,000\) ký tự được lựa chọn dựa trên khả năng xử lý của mô hình embedding, đồng thời đảm bảo đủ độ lớn để bao quát một hoặc vài ý chính hoàn chỉnh. Phần chồng lấn 800 ký tự đóng vai trò như một ``vùng đệm'', giúp hạn chế tình trạng cắt ngang thông tin quan trọng tại ranh giới giữa các đoạn, đặc biệt với những câu hoặc đoạn văn dài dễ bị chia tách một cách không tự nhiên.

Quá trình phân đoạn được thực hiện bởi lớp \texttt{RecursiveCharacterTextSplitter} trong thư viện LangChain -- một công cụ tinh vi cho phép tách văn bản theo mức độ ưu tiên của ký tự phân cách. Thay vì cắt cứng theo vị trí ký tự, bộ phân tách lần lượt ưu tiên các ranh giới tự nhiên của văn bản: đầu tiên là đoạn văn (\texttt{"\textbackslash n\textbackslash n"}), sau đó là câu (\texttt{"\textbackslash n"}), tiếp theo là từ (dấu cách), và cuối cùng mới đến ký tự đơn lẻ nếu không còn lựa chọn nào khác. Cách tiếp cận này giúp bảo toàn ngữ nghĩa và hạn chế tình trạng văn bản bị chia cắt thiếu tự nhiên.

Để cải thiện chất lượng của quá trình phân đoạn, hệ thống bổ sung các cơ chế kiểm soát tự động. Những đoạn quá ngắn (dưới ngưỡng tối thiểu, mặc định là 200 ký tự) hoặc chỉ chứa ký tự trống sẽ bị loại bỏ. Bên cạnh đó, mỗi đoạn văn bản sau khi phân tách đều được gắn kèm siêu dữ liệu quan trọng như tên file nguồn, số trang (đối với tài liệu PDF) và vị trí trong tài liệu gốc. Cách thiết kế này giúp đảm bảo khả năng truy xuất nguồn gốc thông tin trong các bước xử lý tiếp theo.

\subsection{Vector embedding}

Từ các phân đoạn văn bản có cấu trúc thu được, hệ thống tiến hành chuyển đổi chúng thành dạng biểu diễn vector số thông qua quá trình \emph{embedding}. Đây là một trong những bước then chốt trong việc xây dựng khả năng tìm kiếm ngữ nghĩa của hệ thống RAG, bởi chất lượng của các vector embedding ảnh hưởng trực tiếp đến độ chính xác khi truy xuất thông tin liên quan. Mục tiêu của quá trình này là nắm bắt và lưu trữ các mối quan hệ ngữ nghĩa phức tạp giữa từ ngữ, cụm từ và ý tưởng trong một không gian vector nhiều chiều.

Mô hình embedding được nghiên cứu và triển khai là \texttt{text-embedding-3-large} của OpenAI -- một trong những mô hình tiên tiến nhất hiện tại với khả năng xử lý đa ngôn ngữ mạnh mẽ. Mô hình này ánh xạ mỗi đoạn văn bản thành một vector nhúng có độ dài lên đến 3072 chiều, trong đó mỗi chiều biểu diễn một khía cạnh ngữ nghĩa khác nhau của văn bản. Việc lựa chọn \texttt{text-embedding-3-large} không chỉ dựa trên chất lượng embedding vượt trội (đặc biệt đối với nội dung tiếng Việt), mà còn nhờ tính ổn định, khả năng mở rộng của dịch vụ API, cùng sự tương thích tốt với các thư viện LangChain được tích hợp trong hệ thống.

So với các giải pháp embedding khác như \texttt{sentence-transformers} triển khai trực tiếp trên máy chủ, mô hình OpenAI mang lại ưu thế về chất lượng đầu ra và không yêu cầu tài nguyên GPU mạnh mẽ tại hạ tầng cục bộ. Tuy nhiên, điều này cũng đồng nghĩa với việc hệ thống phụ thuộc vào kết nối internet ổn định và phát sinh chi phí API cho mỗi lần gọi embedding. Trong chương trình hiện tại, API key được quản lý thông qua file cấu hình \texttt{.env}, đảm bảo tính bảo mật và tách biệt rõ ràng giữa mã nguồn và dữ liệu nhạy cảm.

Một thành phần quan trọng trong quá trình xây dựng cơ sở dữ liệu vector là cơ chế loại bỏ trùng lặp \emph{(deduplication)}, nhằm tránh lưu trữ nhiều lần những đoạn văn bản giống hệt nhau. Hệ thống sử dụng hàm băm (hash function) SHA-256 để tạo ra dấu định danh (fingerprint) duy nhất cho mỗi đoạn. Khi một phân đoạn mới được xử lý, hệ thống tính toán mã hash của nội dung và so sánh với các giá trị đã tồn tại; nếu trùng khớp, phân đoạn đó sẽ bị loại bỏ khỏi quy trình embedding. Cách tiếp cận này đặc biệt hữu ích khi làm việc với các tài liệu có nội dung chồng chéo (chẳng hạn các phiên bản khác nhau của cùng một báo cáo, hoặc khi người dùng vô tình tải lên cùng một file nhiều lần), giúp tiết kiệm thời gian xử lý, đồng thời giảm đáng kể chi phí API. Tuy nhiên, cần lưu ý rằng phương pháp băm chỉ phát hiện trùng lặp tuyệt đối về mặt ký tự, những đoạn có cùng ý nghĩa nhưng được diễn đạt khác nhau sẽ không bị coi là trùng lặp.

\subsection{Lưu trữ và indexing}

Sau khi thu được các vector nhúng chất lượng cao, hệ thống cần một giải pháp lưu trữ chuyên dụng để quản lý chúng một cách hiệu quả và hỗ trợ truy xuất nhanh chóng. Khác với cơ sở dữ liệu quan hệ truyền thống vốn được thiết kế để lưu trữ dữ liệu dạng bảng, \emph{cơ sở dữ liệu vector} được tối ưu riêng cho việc tìm kiếm trong không gian nhiều chiều. Việc lựa chọn hệ quản trị cơ sở dữ liệu phù hợp không chỉ ảnh hưởng trực tiếp đến hiệu năng tìm kiếm mà còn quyết định khả năng mở rộng và độ ổn định của toàn bộ hệ thống RAG.

Trong dự án này, ChromaDB được lựa chọn làm giải pháp lưu trữ vector nhờ những ưu thế thực tiễn. ChromaDB được thiết kế dưới dạng một cơ sở dữ liệu nhúng \emph{(embedded database)}, có thể tích hợp trực tiếp trong ứng dụng Python mà không cần cài đặt và cấu hình máy chủ cơ sở dữ liệu riêng biệt, rất thuận tiện cho giai đoạn phát triển và vận hành ở quy mô vừa. Bên cạnh đó, ChromaDB hỗ trợ \emph{persistence} -- cơ chế lưu trữ dữ liệu bền vững trên đĩa cứng, giúp bảo toàn toàn bộ cơ sở dữ liệu vector ngay cả khi ứng dụng khởi động lại. Nhờ vậy, hệ thống vừa đảm bảo tính đơn giản trong triển khai, vừa duy trì được độ tin cậy khi vận hành lâu dài.

%-------------checkpoint---------------
So với các giải pháp thương mại như Pinecone hay Weaviate, ChromaDB mang lại lợi thế về chi phí và quyền kiểm soát dữ liệu. Pinecone tuy có hiệu suất cao và khả năng mở rộng tốt, nhưng yêu cầu kết nối internet liên tục và phát sinh chi phí định kỳ dựa trên lượng vector lưu trữ. Weaviate cung cấp nhiều tính năng nâng cao như hybrid search và GraphQL API, tuy nhiên việc cài đặt và vận hành phức tạp hơn đáng kể. ChromaDB cân bằng tốt giữa tính đơn giản, hiệu suất và khả năng tự chủ trong việc quản lý dữ liệu.

Trong code hiện tại, hệ thống khởi tạo ChromaDB với cấu hình persist directory được đặt tại \texttt{Vector\_DB - Documents}, cho phép dữ liệu được lưu trữ cục bộ và tái sử dụng giữa các phiên làm việc. Cơ chế \emph{cập nhật gia tăng} (incremental update) được triển khai thông qua việc theo dõi danh sách các file đã được xử lý trong file \texttt{files.txt}. Khi người dùng thêm tài liệu mới, hệ thống chỉ xử lý những file chưa tồn tại trong cache, tránh việc tính toán lại embedding cho toàn bộ cơ sở dữ liệu.

Quá trình lưu trữ được thực hiện thông qua phương thức \texttt{add\_documents()} của ChromaDB, tự động xử lý việc tạo ID duy nhất cho mỗi vector và duy trì mối liên kết với metadata tương ứng. Metadata đóng vai trò quan trọng trong việc truy xuất thông tin nguồn, bao gồm tên file gốc, đường dẫn và các thông tin bổ sung như số trang đối với tài liệu PDF. Cấu trúc metadata được thiết kế linh hoạt, cho phép mở rộng thêm các trường thông tin khi cần thiết mà không ảnh hưởng đến dữ liệu đã tồn tại.

ChromaDB sử dụng thuật toán indexing tiên tiến để tối ưu hóa tốc độ tìm kiếm trong không gian vector nhiều chiều. Mặc dù chi tiết cài đặt được trừu tượng hóa bởi thư viện, hệ thống tận dụng các kỹ thuật như \emph{Hierarchical Navigable Small World} (HNSW) graphs để thực hiện tìm kiếm láng giềng gần nhất xấp xỉ (approximate nearest neighbor search) với độ phức tạp logarithmic. Điều này cho phép hệ thống duy trì hiệu suất tìm kiếm ổn định ngay cả khi số lượng vector trong cơ sở dữ liệu tăng lên hàng triệu bản ghi.

Khả năng persist của ChromaDB đảm bảo tính bền vững dữ liệu thông qua việc định kỳ ghi xuống đĩa cứng. Phương thức \texttt{persist()} được gọi sau mỗi lần cập nhật để đảm bảo các thay đổi được lưu trữ ngay lập tức. Cơ chế này đặc biệt quan trọng trong môi trường sản xuất, nơi việc mất dữ liệu có thể dẫn đến cần phải xử lý lại toàn bộ tài liệu và tính toán embedding từ đầu, gây tốn kém về thời gian và chi phí API.
